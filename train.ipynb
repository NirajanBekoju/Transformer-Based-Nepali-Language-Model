{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer\n",
    "from config import getConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e220a3-c158-473f-9493-a7cd7b97042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 1024, 'nlayers': 6, 'nhead': 6, 'dropout': 0.2, 'bptt': 64}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 5}\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig()\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  : data/preprocessed_ne_dedup.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/preprocessed_ne_dedup.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    with open('data/ne_dedup.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(\"Preprocessing file\")\n",
    "        text = preProcessText(text)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "else:\n",
    "    print(f\"Reading file  : {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cda573-4fb0-4593-ae9d-e6a80f5d1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319566\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 19566\n"
     ]
    }
   ],
   "source": [
    "train_split = 300_000\n",
    "\n",
    "train_iter = text.split('\\n')[:train_split]\n",
    "test_iter = text.split('\\n')[train_split:]\n",
    "\n",
    "print(len(train_iter), len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ee116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60507\n"
     ]
    }
   ],
   "source": [
    "tokenizer, vocab = getTokenizer()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    # obtain the data in tensor format for each line\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    # concatenate all the lines\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "def batchify(data: Tensor, batch_size: int) -> Tensor:\n",
    "    \"\"\"Divides the data into batch_size separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        batch_size: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // batch_size\n",
    "    data = data[:seq_len * batch_size]\n",
    "    data = data.view(batch_size, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source: Tensor, i: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69512420]) torch.Size([4878548])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13]) tensor([ 2086,  5937,   563,   874, 28794,   356,   530,   396,  7203,   293,\n",
      "            0,   356,   530])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2086, 28794,  7203],\n",
       "        [ 5937,   356,   293],\n",
       "        [  563,   530,     0],\n",
       "        [  874,   396,   356]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['आधिकारिक निर्णयको कारणले', 'वाणिज्य बिभागले', 'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "sample_data = data_process(text)\n",
    "print(sample_data.size(), sample_data)\n",
    "\n",
    "sample_data = batchify(sample_data, 3)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb776cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data = batchify(train_data, bptt).to(device)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a72c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f48849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niranjan/miniconda3/envs/cslr/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    global epoch\n",
    "    global global_step\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(batched_train_data) // bptt\n",
    "    progress_bar = tqdm(enumerate(range(0, batched_train_data.size(0) - 1, bptt)), total=num_batches, desc=f'Epoch {epoch}', ncols=80)\n",
    "    for batch_idx, i in progress_bar:\n",
    "        ### batch_idx -> (1, 2, 3, 4, ...)\n",
    "        ### i -> (0, bptt, 2*bptt, ....)\n",
    "        data, targets = get_batch(batched_train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ## calculate the postfix description for the progress bar\n",
    "        cur_loss = total_loss / (batch_idx + 1)\n",
    "        ppl = math.exp(cur_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": cur_loss, \"ppl\" : ppl}, refresh=True)\n",
    "        \n",
    "        writer.add_scalar('loss/train loss', cur_loss, global_step)\n",
    "        writer.flush()\n",
    "        writer.add_scalar('ppl/train perplexity', ppl, global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "# softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(eval_data) // bptt\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(range(0, eval_data.size(0) - 1, bptt)), total=num_batches, desc=f'Validation {epoch}', ncols=80)\n",
    "        for batch_idx, i in progress_bar:\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    \n",
    "    eval_loss = total_loss / (len(eval_data) - 1)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "\n",
    "    writer.add_scalar('loss/val loss', eval_loss, global_step)\n",
    "    writer.flush()\n",
    "    writer.add_scalar('ppl/val perplexity', eval_ppl, global_step)\n",
    "    writer.flush()\n",
    "\n",
    "    return eval_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_sample_test_corrected.pt\n",
      "3 50913 5.140158006921166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 16971it [44:16,  6.39it/s, loss=5.18, ppl=177]                         \n",
      "Validation 3: 1192it [01:12, 16.44it/s]                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval perplexity : 151.90086732749796\n",
      "saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 16971it [44:14,  6.39it/s, loss=5.07, ppl=160]                         \n",
      "Validation 4: 1192it [01:12, 16.36it/s]                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval perplexity : 140.56843677346157\n",
      "saving the model\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "best_val_loss = float('inf')\n",
    "initial_epoch = 0\n",
    "epochs = app_config[\"epochs\"]\n",
    "global_step = 0\n",
    "best_model = None\n",
    "\n",
    "# preload the model if exists to train more epochs\n",
    "best_model_path = 'models/best_model.pt'\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Preloading model {best_model_path}\")\n",
    "    state = torch.load(best_model_path)\n",
    "    \n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "    best_val_loss = state['best_val_loss']\n",
    "    \n",
    "    print(initial_epoch, global_step, best_val_loss)\n",
    "\n",
    "# initializing the tensorbaord log writer\n",
    "writer = SummaryWriter(app_config[\"logs\"])\n",
    "\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, batched_test_data)\n",
    "\n",
    "    # save the model if validation loss decreases\n",
    "\n",
    "    if eval_loss < best_val_loss:\n",
    "        print(f\"eval perplexity : {math.exp(eval_loss)}\")\n",
    "        print(\"saving the model\")\n",
    "        best_val_loss = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "        directory_path = 'models'\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step, \n",
    "                'best_val_loss' : best_val_loss,\n",
    "            }, os.path.join(directory_path, 'best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d9916-268a-464c-a255-a06ad1c1136f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cslr",
   "language": "python",
   "name": "cslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
