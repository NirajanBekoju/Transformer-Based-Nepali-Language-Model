{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer\n",
    "from config import getConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e220a3-c158-473f-9493-a7cd7b97042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 1024, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'bptt': 64}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 25}\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig()\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  : data/preprocessed_ne_dedup.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/preprocessed_ne_dedup.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    with open('data/ne_dedup.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(\"Preprocessing file\")\n",
    "        text = preProcessText(text)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "else:\n",
    "    print(f\"Reading file  : {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cda573-4fb0-4593-ae9d-e6a80f5d1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319566\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000 19566\n"
     ]
    }
   ],
   "source": [
    "train_split = 300_000\n",
    "\n",
    "train_iter = text.split('\\n')[:train_split]\n",
    "test_iter = text.split('\\n')[train_split:]\n",
    "\n",
    "print(len(train_iter), len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ee116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60507\n"
     ]
    }
   ],
   "source": [
    "tokenizer, vocab = getTokenizer()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    # obtain the data in tensor format for each line\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    # concatenate all the lines\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "def batchify(data: Tensor, batch_size: int) -> Tensor:\n",
    "    \"\"\"Divides the data into batch_size separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        batch_size: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // batch_size\n",
    "    data = data[:seq_len * batch_size]\n",
    "    data = data.view(batch_size, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source: Tensor, i: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69512420]) torch.Size([4878548])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13]) tensor([ 2086,  5937,   563,   874, 28794,   356,   530,   396,  7203,   293,\n",
      "            0,   356,   530])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2086, 28794,  7203],\n",
       "        [ 5937,   356,   293],\n",
       "        [  563,   530,     0],\n",
       "        [  874,   396,   356]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['आधिकारिक निर्णयको कारणले', 'वाणिज्य बिभागले', 'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "sample_data = data_process(text)\n",
    "print(sample_data.size(), sample_data)\n",
    "\n",
    "sample_data = batchify(sample_data, 3)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb776cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data = batchify(train_data, bptt).to(device)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a72c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f48849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niranjan/miniconda3/envs/cslr/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    global epoch\n",
    "    global global_step\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(batched_train_data) // bptt\n",
    "    progress_bar = tqdm(enumerate(range(0, batched_train_data.size(0) - 1, bptt)), total=num_batches, desc=f'Epoch {epoch}', ncols=80)\n",
    "    for batch_idx, i in progress_bar:\n",
    "        ### batch_idx -> (1, 2, 3, 4, ...)\n",
    "        ### i -> (0, bptt, 2*bptt, ....)\n",
    "        data, targets = get_batch(batched_train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ## calculate the postfix description for the progress bar\n",
    "        cur_loss = total_loss / (batch_idx + 1)\n",
    "        ppl = math.exp(cur_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": cur_loss, \"ppl\" : ppl}, refresh=True)\n",
    "        \n",
    "        writer.add_scalar('loss/train loss', cur_loss, global_step)\n",
    "        writer.flush()\n",
    "        writer.add_scalar('ppl/train perplexity', ppl, global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "# softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(eval_data) // bptt\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(range(0, eval_data.size(0) - 1, bptt)), total=num_batches, desc=f'Validation {epoch}', ncols=80)\n",
    "        for batch_idx, i in progress_bar:\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    \n",
    "    eval_loss = total_loss / (len(eval_data) - 1)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "\n",
    "    writer.add_scalar('loss/val loss', eval_loss, global_step)\n",
    "    writer.flush()\n",
    "    writer.add_scalar('ppl/val perplexity', eval_ppl, global_step)\n",
    "    writer.flush()\n",
    "\n",
    "    return eval_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_sample_test_corrected.pt\n",
      "5 235 8.316574757146686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|       | 45/16970 [00:07<44:00,  6.41it/s, loss=8.23, ppl=3.73e+3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(app_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(initial_epoch, epochs):\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, batched_test_data)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# save the model if validation loss decreases\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m## calculate the postfix description for the progress bar\u001b[39;00m\n\u001b[1;32m     28\u001b[0m cur_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "best_val_loss = float('inf')\n",
    "initial_epoch = 0\n",
    "epochs = app_config[\"epochs\"]\n",
    "global_step = 0\n",
    "best_model = None\n",
    "\n",
    "# preload the model if exists to train more epochs\n",
    "best_model_path = 'models/best_model_sample_test_corrected.pt'\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Preloading model {best_model_path}\")\n",
    "    state = torch.load(best_model_path)\n",
    "    \n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "    best_val_loss = state['best_val_loss']\n",
    "    \n",
    "    print(initial_epoch, global_step, best_val_loss)\n",
    "\n",
    "# initializing the tensorbaord log writer\n",
    "writer = SummaryWriter(app_config[\"logs\"])\n",
    "\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, batched_test_data)\n",
    "\n",
    "    # save the model if validation loss decreases\n",
    "    if eval_loss < best_val_loss:\n",
    "        best_val_loss = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "        directory_path = 'models'\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step, \n",
    "                'best_val_loss' : best_val_loss,\n",
    "            }, os.path.join(directory_path, 'best_model_sample_test_corrected.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d9916-268a-464c-a255-a06ad1c1136f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cslr",
   "language": "python",
   "name": "cslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
