{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ff6fd7-900e-4620-b369-9deb69d97f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer\n",
    "from config import getConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80ad5ff-c38b-4c9c-8326-58593e0dbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model\n",
    "\n",
    "def loadModel(best_model_path):\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Preloading model {best_model_path}\")\n",
    "        state = torch.load(best_model_path)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Model Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c00f79-716d-41fc-b027-74418c542ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 1024, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'bptt': 64}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 25}\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niranjan/miniconda3/envs/cslr/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig()\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "tokenizer, vocab = getTokenizer()\n",
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb36cce0-0ff7-4b8b-a128-2fb116ee6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_sample_test_corrected.pt\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'models/best_model_sample_test_corrected.pt'\n",
    "loaded_model = loadModel(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188ea08e-141e-4ec5-b04d-d1c790712f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    # obtain the data in tensor format for each line\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    # concatenate all the lines\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "def batchify(data: Tensor, batch_size: int) -> Tensor:\n",
    "    \"\"\"Divides the data into batch_size separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        batch_size: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // batch_size\n",
    "    data = data[:seq_len * batch_size]\n",
    "    data = data.view(batch_size, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    \n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    gen_data = gen_data.to(device)\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "\n",
    "        indices = torch.topk(output_softmax_permuted,10 ,dim=2).indices.squeeze(0)\n",
    "        values = torch.topk(softmax(output_softmax_permuted),10 ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "\n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "        # print('next word: ', [vocab.lookup_token(next_index)],'values: ',values.squeeze(0)[-1])\n",
    "        pred_text.append([vocab.lookup_token((next_index))][0])\n",
    "        \n",
    "        if(batch_size <= 10):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe07873-a781-4f46-ba42-54725ff9681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words=5, k=50):\n",
    "    model.eval()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        # print('i:', i)\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.topk(output_softmax_permuted, k, dim=2).indices.squeeze(0)\n",
    "\n",
    "        values = torch.topk(softmax(output_softmax_permuted), k, dim=2).values\n",
    "        values = values/torch.sum(values, dim=2, keepdims=True)\n",
    "\n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "\n",
    "        pred_text.append([vocab.lookup_token(next_index)][0])\n",
    "        if(batch_size < 15):\n",
    "            gen_data = torch.cat((gen_data[:, :], next_index.unsqueeze(0).unsqueeze(0)), 0)\n",
    "            batch_size = gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:, :], next_index.unsqueeze(0).unsqueeze(0)), 0)\n",
    "            batch_size = gen_data.size(0)\n",
    "\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52b85173-c375-4176-baee-3810f120ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) tensor([2086, 5937,  563])\n",
      "आधिकारिक निर्णयको कारणले <num> । । <num> पनि , । तर <num> छ\n"
     ]
    }
   ],
   "source": [
    "text = ['आधिकारिक निर्णयको कारणले']\n",
    "# text = ['आधिकारिक निर्णयको']\n",
    "sample_data = data_process(text)\n",
    "print(sample_data.size(), sample_data)\n",
    "sample_data = batchify(sample_data, 3)\n",
    "\n",
    "z = generator(loaded_model, sample_data[:,-1].unsqueeze(1),no_words = 10)\n",
    "print(text[0] + ' ' + ' '.join(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b70552e5-1bb7-46ee-9ccb-2cfeeda57580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['गर्दै', '।', 'हुन्', '।', 'हुन्', '।', 'छ', '।', 'तथा', '<num>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonnaive_generator(loaded_model,  sample_data[:,-1].unsqueeze(1), no_words=10, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e137152-9f09-4396-9b3b-527f66fdfb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cslr",
   "language": "python",
   "name": "cslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
