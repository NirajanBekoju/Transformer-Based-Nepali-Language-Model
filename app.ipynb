{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ff6fd7-900e-4620-b369-9deb69d97f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer\n",
    "from config import getConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80ad5ff-c38b-4c9c-8326-58593e0dbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model\n",
    "\n",
    "def loadModel(best_model_path):\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Preloading model {best_model_path}\")\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.load(best_model_path)\n",
    "        else:\n",
    "            state = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Model Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c00f79-716d-41fc-b027-74418c542ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 1024, 'nlayers': 6, 'nhead': 6, 'dropout': 0.2, 'bptt': 64}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 25}\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirajan/miniconda3/envs/cslr/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig()\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "tokenizer, vocab = getTokenizer()\n",
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb36cce0-0ff7-4b8b-a128-2fb116ee6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'models/best_model.pt'\n",
    "loaded_model = loadModel(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188ea08e-141e-4ec5-b04d-d1c790712f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    # obtain the data in tensor format for each line\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    # concatenate all the lines\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "def batchify(data: Tensor, batch_size: int) -> Tensor:\n",
    "    \"\"\"Divides the data into batch_size separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        batch_size: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // batch_size\n",
    "    data = data[:seq_len * batch_size]\n",
    "    data = data.view(batch_size, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    \n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    gen_data = gen_data.to(device)\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "\n",
    "        indices = torch.topk(output_softmax_permuted,10 ,dim=2).indices.squeeze(0)\n",
    "        values = torch.topk(softmax(output_softmax_permuted),10 ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "\n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "        # print('next word: ', [vocab.lookup_token(next_index)],'values: ',values.squeeze(0)[-1])\n",
    "        pred_text.append([vocab.lookup_token((next_index))][0])\n",
    "        \n",
    "        if(batch_size <= 10):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe07873-a781-4f46-ba42-54725ff9681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words=5, k=50):\n",
    "    model.eval()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "            \n",
    "        # generate the probability of the next word\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        \n",
    "        # obtain the \"k\" top probable words index\n",
    "        # both indices and values are of size (no. of words, k=50)\n",
    "        indices = torch.topk(output_softmax_permuted, k, dim=2).indices.squeeze(0)\n",
    "        # obtain the top \"k\" probability of the probable words\n",
    "        values = torch.topk(softmax(output_softmax_permuted), k, dim=2).values\n",
    "        values = values/torch.sum(values, dim=2, keepdims=True)\n",
    "        values = values.squeeze(0)        \n",
    "        \n",
    "        # create categorical distribution and take sample from values\n",
    "        # categorical distribution take 1 sample from k=50 samples of each dimension\n",
    "        for _ in range(10): \n",
    "            ind_sampled = torch.distributions.Categorical(values).sample()\n",
    "            next_index = indices[-1][ind_sampled[-1]]\n",
    "            # if the obtained token is not <unk>, then no need to sample again\n",
    "            if vocab.lookup_token(next_index) != '<unk>':\n",
    "                break\n",
    "            \n",
    "        pred_text.append([vocab.lookup_token(next_index)][0])\n",
    "        if(batch_size < 15):\n",
    "            gen_data = torch.cat((gen_data[:, :], next_index.unsqueeze(0).unsqueeze(0)), 0)\n",
    "            batch_size = gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:, :], next_index.unsqueeze(0).unsqueeze(0)), 0)\n",
    "            batch_size = gen_data.size(0)\n",
    "\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b85173-c375-4176-baee-3810f120ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) tensor([ 77, 138])\n",
      "tensor([[ 77],\n",
      "        [138]])\n",
      "म घर <unk> लागि पनि <unk> हुँ । मलाई पनि थाहा थिएन\n"
     ]
    }
   ],
   "source": [
    "text = ['आधिकारिक निर्णयको कारणले']\n",
    "text = ['म घर जान']\n",
    "text = ['म घर']\n",
    "\n",
    "sample_data = data_process(text)\n",
    "print(sample_data.size(), sample_data)\n",
    "sample_data = batchify(sample_data, 1)\n",
    "print(sample_data)\n",
    "\n",
    "z = generator(loaded_model, sample_data[:,-1].unsqueeze(1),no_words = 10)\n",
    "print(text[0] + ' ' + ' '.join(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b70552e5-1bb7-46ee-9ccb-2cfeeda57580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "म घर फर्किने तयारीमा छु । तर , त्यो क्षण मेरा दुई\n"
     ]
    }
   ],
   "source": [
    "pred_text = nonnaive_generator(loaded_model,  sample_data[:,-1].unsqueeze(1), no_words=10, k=50)\n",
    "output = text[0] + ' ' + ' '.join(pred_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da59be0",
   "metadata": {},
   "source": [
    "## Gradio application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6ded4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predText(text : str, num_words : int):\n",
    "    text = [text]\n",
    "    num_words = int(num_words)\n",
    "    sample_data = data_process(text)\n",
    "    sample_data = batchify(sample_data, 1)\n",
    "    pred_text = nonnaive_generator(loaded_model,  sample_data[:,-1].unsqueeze(1), no_words=num_words, k=50)\n",
    "    whole_text = text[0] + ' ' + ' '.join(pred_text)\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04f6707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'म घर नजिकैको नजिकैको अस्पतालमा उपचार भइरहेको छ । <num> वर्षदेखि उपचार गराइरहेकी एक सय <num> वर्षिया छोरी हुन् । उनले तीन वर्षअघि मिर्गौला प्रत्यारोपण गरेको र तीन वर्षअघि काठमाडौं , <num> माघ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"म घर नजिकैको\"\n",
    "num_words = 30\n",
    "predText(text, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5383b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirajan/miniconda3/envs/cslr/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f38739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Gradio Interface with custom HTML and JavaScript\n",
    "input_text_box = gr.Textbox(label=\"Text\", value=\"म घर\", lines=5)\n",
    "\n",
    "def predText(text : str, num_words : int):\n",
    "    global input_text_box\n",
    "    text = [text]\n",
    "    num_words = int(num_words)\n",
    "    sample_data = data_process(text)\n",
    "    sample_data = batchify(sample_data, 1)\n",
    "    pred_text = nonnaive_generator(loaded_model,  sample_data[:,-1].unsqueeze(1), no_words=num_words, k=50)\n",
    "    whole_text = text[0] + ' ' + ' '.join(pred_text)\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afd6784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [[\"म घर\", 10], [\"मलाई\", 40], [\"आज\", 70]]\n",
    "with gr.Blocks() as interface:\n",
    "    interface.title = \"Nepali Text Generation Model\"\n",
    "    gr.Markdown(\"# Nepali Text Generation\")\n",
    "    gr.Markdown(\"Start typing nepali below and then click **Submit** to generate text.\")\n",
    "    gr.Markdown(\"You can select examples from the table below and then click **Submit** to generate text.\")\n",
    "    \n",
    "    input_text_box = gr.Textbox(label=\"Text\", value=\"म घर\", lines=5, placeholder=\"Enter Nepali Text\")\n",
    "    input_num_words = gr.Number(label=\"Number of word to generate\", value=5)\n",
    "\n",
    "    btn = gr.Button(value=\"Submit\")\n",
    "    \n",
    "    btn.click(predText, inputs=[input_text_box, input_num_words], outputs=[input_text_box])\n",
    "    \n",
    "    gr.Examples(examples=examples, inputs=[input_text_box, input_num_words], outputs=[input_text_box])\n",
    "    gr.flagging = True\n",
    "    \n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392832a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cslr",
   "language": "python",
   "name": "cslr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
