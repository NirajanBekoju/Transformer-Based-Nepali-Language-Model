{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4348b2f9",
   "metadata": {},
   "source": [
    "# Import Torch functions and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer,try_gpu \n",
    "from config import getConfig\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d092fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 1024, 'nlayers': 6, 'nhead': 6, 'dropout': 0.2, 'bptt': 64}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 25}\n"
     ]
    }
   ],
   "source": [
    "# model_config, app_config = getConfig(small = True)\n",
    "model_config, app_config = getConfig()\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "device = try_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba983f7",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  : data/preprocessed_ne_dedup_no_num.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/preprocessed_ne_dedup_no_num.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    with open('data/ne_dedup.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(\"Preprocessing file\")\n",
    "        text = preProcessText(text,tokenizer_type = 'bpe')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "else:\n",
    "    print(f\"Reading file  : {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b02d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341961"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 300000\n",
    "\n",
    "train_iter_first = text.split('\\n')[:train_split]\n",
    "test_iter = text.split('\\n')[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d50ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer,vocab = getTokenizer(tokenizer_type = 'bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8092b820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = tokenizer.get_vocab()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd18bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded $tring:  ['à¤®à¤¹', 'à¤¾', 'à¤¨', 'à¤¾', 'à¤¯à¤ķ', 'Ġà¤°', 'à¤¾', 'à¤ľ', 'à¥ĩ', 'à¤¶', 'Ġà¤¹à¤®', 'à¤¾', 'à¤²', 'Ġà¤ħà¤¹', 'à¤¿', 'à¤²', 'à¥ĩ', 'Ġà¤ļà¤²à¤ļ', 'à¤¿', 'à¤¤', 'à¥į', 'à¤°', 'Ġà¤ķ', 'à¥į', 'à¤·', 'à¥ĩ', 'à¤¤', 'à¥į', 'à¤°à¤®', 'à¤¾', 'Ġà¤ª', 'à¤¾', 'à¤¤à¤²', 'à¤¿', 'à¤ı', 'Ġà¥¤']\n",
      "Decoded $tring: महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try the encoder and decoder\n",
    "l = tokenizer.encode('महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।').tokens\n",
    "l_ = tokenizer.encode('महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।').ids\n",
    "\n",
    "print(\"Encoded $tring: \",l)\n",
    "print(\"Decoded $tring:\",tokenizer.decode(l_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8771c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded $tring:  ['à¤¹', 'à¤¾', 'à¤¤à¤®', 'à¤¾', 'Ġà¤¤', 'à¥į', 'à¤°', 'à¤¿', 'à¤¶', 'à¥ģ', 'à¤²', 'Ġà¤ľà¤Ł', 'à¤¾', 'Ġà¤®', 'à¥ģ', 'à¤ķ', 'à¥ģ', 'à¤Ł', 'Ġà¤¶', 'à¥ģ', 'à¤¶', 'à¥ĭ', 'à¤Ń', 'à¥Ģ', 'à¤¤', 'Ġà¤¬', 'à¥į', 'à¤°à¤®', 'à¥į', 'à¤¹', 'à¤¾', 'Ġà¤īà¤¤', 'à¥į', 'à¤ªà¤¤', 'à¤¿', 'Ġà¤¹', 'à¥ģ', 'à¤¨', 'à¥ģ', 'Ġà¥¤']\n",
      "Encoded id$:  [277, 259, 471, 259, 303, 260, 261, 264, 309, 271, 267, 1453, 259, 291, 271, 263, 271, 287, 333, 271, 309, 266, 320, 272, 269, 289, 260, 328, 260, 277, 259, 436, 260, 407, 264, 295, 271, 262, 271, 281]\n",
      "Decoded $tring: हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try the encoder and decoder\n",
    "l = tokenizer.encode('हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।').tokens\n",
    "l_ = tokenizer.encode('हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।').ids\n",
    "\n",
    "print(\"Encoded $tring: \",l)\n",
    "print(\"Encoded id$: \",l_)\n",
    "print(\"Decoded $tring:\",tokenizer.decode(l_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf967495",
   "metadata": {},
   "source": [
    "#  some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f7919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(l):\n",
    "    splitted_list = []\n",
    "    z = 0\n",
    "    for i,idx in enumerate(l):\n",
    "        if idx == 220:\n",
    "            splitted_list.append(l[z:i])\n",
    "            z = i+1\n",
    "    if z <= len(l)-1:\n",
    "        splitted_list.append(l[z:])\n",
    "    return splitted_list\n",
    "\n",
    "def splits_to_token(splited_list):\n",
    "    strings = [tokenizer.decode(l) for l in splited_list]\n",
    "    \n",
    "    return strings\n",
    "\n",
    "# print(tokenizer.encode(' ').ids)\n",
    "\n",
    "tokenizer.decode([978,\n",
    " 261,\n",
    " 264,\n",
    "624,\n",
    " 261,\n",
    " 263])\n",
    "\n",
    "k = split_list(l_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d435fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_to_token(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(tokenizer.encode(item).ids, dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "#     data = data.view(bsz, seq_len).t().contiguous()\n",
    "    data = data.view(bsz,seq_len).t()\n",
    "#     return data.to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "# seq_length = 128\n",
    "import math\n",
    "\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Split\n",
    "train_data = data_process(train_iter_first)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5674c13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275240667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d40be4",
   "metadata": {},
   "source": [
    "# Working with a dummy Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "\n",
    "\n",
    "text = ['आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "#text = ['जनसंख्या']\n",
    "sample_data = data_process(\n",
    "    text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c88184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3022,  264,  263,  259,  261,  264,  263,  288,  264,  261,  260, 2151,\n",
       "         266,  282,  259,  885,  265,  220,   11,  297,  259,  316,  264,  293,\n",
       "         260,  270,  289,  264,  320,  259,  505,  265,  220,   11,  274,  311,\n",
       "         270,  271,  263,  260,  269,  285,  259,  293,  260,  270,  545,  265,\n",
       "         261,  264,  263,  272,  325,  271,  279,  260,  261,  272,  276,  259,\n",
       "         262,  272,  288,  264,  261,  260,  268,  259,  269,  259,  279,  260,\n",
       "         280,  259,  261,  259,  274,  311,  270,  271,  263,  260,  269,  285,\n",
       "         259,  293,  260,  270])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9de8a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3022,  260],\n",
       "        [ 264,  270],\n",
       "        [ 263,  545],\n",
       "        [ 259,  265],\n",
       "        [ 261,  261],\n",
       "        [ 264,  264],\n",
       "        [ 263,  263],\n",
       "        [ 288,  272],\n",
       "        [ 264,  325],\n",
       "        [ 261,  271],\n",
       "        [ 260,  279],\n",
       "        [2151,  260],\n",
       "        [ 266,  261],\n",
       "        [ 282,  272],\n",
       "        [ 259,  276],\n",
       "        [ 885,  259],\n",
       "        [ 265,  262],\n",
       "        [ 220,  272],\n",
       "        [  11,  288],\n",
       "        [ 297,  264],\n",
       "        [ 259,  261],\n",
       "        [ 316,  260],\n",
       "        [ 264,  268],\n",
       "        [ 293,  259],\n",
       "        [ 260,  269],\n",
       "        [ 270,  259],\n",
       "        [ 289,  279],\n",
       "        [ 264,  260],\n",
       "        [ 320,  280],\n",
       "        [ 259,  259],\n",
       "        [ 505,  261],\n",
       "        [ 265,  259],\n",
       "        [ 220,  274],\n",
       "        [  11,  311],\n",
       "        [ 274,  270],\n",
       "        [ 311,  271],\n",
       "        [ 270,  263],\n",
       "        [ 271,  260],\n",
       "        [ 263,  269],\n",
       "        [ 260,  285],\n",
       "        [ 269,  259],\n",
       "        [ 285,  293],\n",
       "        [ 259,  260],\n",
       "        [ 293,  270]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = batchify(sample_data, 2)\n",
    "print(\"Given word:\", text[0])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd7744",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee046362",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data = batchify(train_data, bptt).to(device)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6290c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4619fe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2635234304"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1da52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf7ec3",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "softmax = nn.Softmax(dim=2)\n",
    "#softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    global epoch\n",
    "    global global_step\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(batched_train_data) // bptt\n",
    "    progress_bar = tqdm(enumerate(range(0, batched_train_data.size(0) - 1, bptt)), total=num_batches, desc=f'Epoch {epoch}', ncols=80)\n",
    "    for batch_idx, i in progress_bar:\n",
    "        ### batch_idx -> (1, 2, 3, 4, ...)\n",
    "        ### i -> (0, bptt, 2*bptt, ....)\n",
    "        data, targets = get_batch(batched_train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ## calculate the postfix description for the progress bar\n",
    "        cur_loss = total_loss / (batch_idx + 1)\n",
    "        ppl = math.exp(cur_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": cur_loss, \"ppl\" : ppl}, refresh=True)\n",
    "        \n",
    "        writer.add_scalar('loss/train loss', cur_loss, global_step)\n",
    "        writer.flush()\n",
    "        writer.add_scalar('ppl/train perplexity', ppl, global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(eval_data) // bptt\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(range(0, eval_data.size(0) - 1, bptt)), total=num_batches, desc=f'Validation {epoch}', ncols=80)\n",
    "        for batch_idx, i in progress_bar:\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    \n",
    "    eval_loss = total_loss / (len(eval_data) - 1)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "\n",
    "    writer.add_scalar('loss/val loss', eval_loss, global_step)\n",
    "    writer.flush()\n",
    "    writer.add_scalar('ppl/val perplexity', eval_ppl, global_step)\n",
    "    writer.flush()\n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73f1d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a41c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'models/best_model_bpe.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f59064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_bpe.pt\n",
      "2 134396 1.822638274364511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 67198it [4:48:14,  3.89it/s, loss=1.89, ppl=6.59]                      \n",
      "Validation 2: 10067it [19:36,  8.56it/s]                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval perplexity : 5.987692215811574\n",
      "saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  36%|█  | 24104/67197 [1:50:36<3:17:27,  3.64it/s, loss=1.87, ppl=6.48]"
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "best_val_loss = float('inf')\n",
    "initial_epoch = 0\n",
    "epochs = app_config[\"epochs\"]\n",
    "global_step = 0\n",
    "best_model = None\n",
    "\n",
    "# preload the model if exists to train more epochs\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Preloading model {best_model_path}\")\n",
    "    state = torch.load(best_model_path)\n",
    "    \n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "    best_val_loss = state['best_val_loss']\n",
    "    \n",
    "    print(initial_epoch, global_step, best_val_loss)\n",
    "\n",
    "# initializing the tensorbaord log writer\n",
    "writer = SummaryWriter(app_config[\"logs\"])\n",
    "\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, batched_test_data)\n",
    "\n",
    "    # save the model if validation loss decreases\n",
    "\n",
    "    if eval_loss < best_val_loss:\n",
    "        print(f\"eval perplexity : {math.exp(eval_loss)}\")\n",
    "        print(\"saving the model\")\n",
    "        best_val_loss = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "        directory_path = 'models'\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step, \n",
    "                'best_val_loss' : best_val_loss,\n",
    "            }, os.path.join(directory_path, 'best_model_wp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsoftmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33185f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_to_token(x):\n",
    "    token_list = [tokenizer.id_to_token(id) for id in x]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b999611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', '0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token([10,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4c44",
   "metadata": {},
   "source": [
    "# sample Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc09b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_wp.pt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Not Found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 9\u001b[0m, in \u001b[0;36mloadModel\u001b[1;34m(best_model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m         state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "def loadModel(best_model_path):\n",
    "    global model\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Preloading model {best_model_path}\")\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.load(best_model_path)\n",
    "        else:\n",
    "            state = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Model Not Found\")\n",
    "        \n",
    "loaded_model = loadModel(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2c2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = model\n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "\n",
    "        pred_text.append(indices[0][-1])\n",
    "        if(batch_size < 128):\n",
    "            gen_data = torch.cat((gen_data[:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text\n",
    "\n",
    "\n",
    "\n",
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words = 5,k=50):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.topk(output_softmax_permuted,k ,dim=2).indices.squeeze(0)\n",
    "        \n",
    "        values = torch.topk(softmax(output_softmax_permuted),k ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "        \n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "\n",
    "        pred_text.append(next_index.item())\n",
    "        if(batch_size < 128):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c93c46c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 1]),\n",
       " tensor([[262],\n",
       "         [265],\n",
       "         [278],\n",
       "         [259],\n",
       "         [389],\n",
       "         [259],\n",
       "         [523],\n",
       "         [271],\n",
       "         [262],\n",
       "         [264],\n",
       "         [263]], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = ['नेपालमा आधुनिक']\n",
    "st_i = data_process(st)\n",
    "st_i = st_i.unsqueeze(1).to(device)\n",
    "st_i.shape,st_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08777245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नेपालमा आधुनिकृकाना साको ।ान्र्यामाकामान्कै संसुनी संको हो स'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = nonnaive_generator(loaded_model, st_i,no_words = 40, k=10)\n",
    "m = splits_to_token(split_list(z))\n",
    "' '.join(st)+ ' '.join(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b771ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ृकाना साको ।ान्र्यामाकामान्कै संसुनी संको हो स'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62ae80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
