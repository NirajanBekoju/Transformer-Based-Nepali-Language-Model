{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea56036d",
   "metadata": {},
   "source": [
    "# Import Torch functions and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer,try_gpu , word_piece_decoder, word_piece_encoder\n",
    "from config import getConfig\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64201d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 800, 'nlayers': 4, 'nhead': 4, 'dropout': 0.05, 'bptt': 32}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig(small = True)\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "device = try_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba983f7",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740ae401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  : data/preprocessed_morph.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = 'data/preprocessed_morph.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Run morpheme_datagen notebook\")\n",
    "else:\n",
    "    print(f\"Reading file  : {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = preProcessText(text,tokenizer_type = 'morpheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59c6611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'बर्दिबास * नगरपालिका को * तेस्रो नगर परिषदबाट पारित आव २०७३ * । ७४ को संशोधित र * २०७४ * । ७५ को प्रस्तावित नीति ? * कार्यक्रम * तथा * बजेट\\nअार्थिक * वर्ष * २०७५७६ * काे नदिजन्य * पदार्थ काे * उत्खनन् गरी बिक्रि * वितरण * तथा अान्तरिक निकासी गर्ने * कार्य काे * बाेलपत्र सम्बन्धी * सुचना\\nसक्ष ार सप्तरी अभियानमा सप्तरीबासी * सम्पूर्ण * सरोकारवालाहरु को * सहयोग र * सहभागिता का ो लागि अनुराोध छ । * सामुदायिक * अध्ययन * केन्द्र हरूको * नविकरण सम्बन्धमा । \\nकाठमाडौं * ? १२ कातिक । * राष्ट्रपति * विद्या'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b02d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123563"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_first = text.split('\\n')[:100000]\n",
    "test_iter = text.split('\\n')[100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781400f",
   "metadata": {},
   "source": [
    "# Run Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006d0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "\n",
    "# tokenizer = get_tokenizer(None)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(\n",
    "#     map(tokenizer, train_iter_first), specials=['<unk>'],max_tokens = 30000)\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "\n",
    "# # Save for first time\n",
    "# with open('transformer_vocab_morpheme.pickle','wb') as f:\n",
    "#     pickle.dump(vocab,f)\n",
    "\n",
    "tokenizer,vocab = getTokenizer(tokenizer_type = 'morpheme')\n",
    "\n",
    "# with open('tokenizers/transformer_vocab_morpheme.pickle','rb') as f:\n",
    "#     vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8092b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c622c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bb6300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('गराउन', 630),\n",
       " ('तामाकोशी', 7912),\n",
       " ('धानको', 5649),\n",
       " ('कम्तीमा', 1348),\n",
       " ('सञ्चार', 1107),\n",
       " ('छिटै', 6908),\n",
       " ('आणविक', 5099),\n",
       " ('इलाका', 2201),\n",
       " ('राजपत्रमा', 13387),\n",
       " ('वर्षीय', 625),\n",
       " ('सम्भावना', 482),\n",
       " ('विकको', 11980),\n",
       " ('युनियनका', 11859),\n",
       " ('दावी', 1923),\n",
       " ('ठूलै', 4034),\n",
       " ('गरेका', 31),\n",
       " ('मैतीदेवी', 28939),\n",
       " ('हाँसो', 7269),\n",
       " ('अपरेटर', 13993),\n",
       " ('पासो', 19681)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.get_stoi().items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "78f4391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c810f3",
   "metadata": {},
   "source": [
    "#  some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "#     data = data.view(bsz, seq_len).t().contiguous()\n",
    "    data = data.view(bsz,seq_len).t()\n",
    "#     return data.to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "seq_length = 128\n",
    "def get_batch(source: Tensor, i: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(seq_length, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Split\n",
    "train_data = data_process(train_iter_first)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5674c13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31139407"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14037,     1,   146,     4,     1,   572,   778,     0,   985,  1879,\n",
       "         1520,     1,     2,  3696,     4, 11933,     5,     1,   813,     1,\n",
       "            2,  1527,     4,  4469,   366,     3,     1,    71,     1,    20,\n",
       "            1,   403, 25615,     1,    64,     1,  5413,     1,  1624,     0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d40be4",
   "metadata": {},
   "source": [
    "# Working with a dummy Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "\n",
    "\n",
    "text = ['आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "#text = ['जनसंख्या']\n",
    "sample_data = data_process(\n",
    "    text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c88184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2365, 9109,  666,    0, 1027,    0,    0,  422,  388,  474, 7705,  315,\n",
       "           0,  422,  388])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9de8a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2365,  422],\n",
       "        [9109,  388],\n",
       "        [ 666,  474],\n",
       "        [   0, 7705],\n",
       "        [1027,  315],\n",
       "        [   0,    0],\n",
       "        [   0,  422]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = batchify(sample_data, 2)\n",
    "print(\"Given word:\", text[0])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd7744",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43358ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data = batchify(train_data, bptt).to(device)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "551e4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59077853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401597952"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee046362",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_train_data = batchify(train_data, bptt)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1da52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf7ec3",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83f7a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "softmax = nn.Softmax(dim=2)\n",
    "#softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c7cfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    global epoch\n",
    "    global global_step\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(batched_train_data) // bptt\n",
    "    progress_bar = tqdm(enumerate(range(0, batched_train_data.size(0) - 1, bptt)), total=num_batches, desc=f'Epoch {epoch}', ncols=80)\n",
    "    for batch_idx, i in progress_bar:\n",
    "        ### batch_idx -> (1, 2, 3, 4, ...)\n",
    "        ### i -> (0, bptt, 2*bptt, ....)\n",
    "        data, targets = get_batch(batched_train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ## calculate the postfix description for the progress bar\n",
    "        cur_loss = total_loss / (batch_idx + 1)\n",
    "        ppl = math.exp(cur_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": cur_loss, \"ppl\" : ppl}, refresh=True)\n",
    "        \n",
    "        writer.add_scalar('loss/train loss', cur_loss, global_step)\n",
    "        writer.flush()\n",
    "        writer.add_scalar('ppl/train perplexity', ppl, global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb16cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(eval_data) // bptt\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(range(0, eval_data.size(0) - 1, bptt)), total=num_batches, desc=f'Validation {epoch}', ncols=80)\n",
    "        for batch_idx, i in progress_bar:\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    \n",
    "    eval_loss = total_loss / (len(eval_data) - 1)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "\n",
    "    writer.add_scalar('loss/val loss', eval_loss, global_step)\n",
    "    writer.flush()\n",
    "    writer.add_scalar('ppl/val perplexity', eval_ppl, global_step)\n",
    "    writer.flush()\n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "#softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73f1d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29881c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'models/best_model_mp.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▎      | 1324/30409 [04:20<1:35:26,  5.08it/s, loss=6.34, ppl=569]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(app_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(initial_epoch, epochs):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, batched_test_data)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# save the model if validation loss decreases\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens), targets)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mF:\\LM bytepair\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\LM bytepair\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "best_val_loss = float('inf')\n",
    "initial_epoch = 0\n",
    "epochs = app_config[\"epochs\"]\n",
    "global_step = 0\n",
    "best_model = None\n",
    "\n",
    "# preload the model if exists to train more epochs\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Preloading model {best_model_path}\")\n",
    "    state = torch.load(best_model_path)\n",
    "    \n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "    best_val_loss = state['best_val_loss']\n",
    "    \n",
    "    print(initial_epoch, global_step, best_val_loss)\n",
    "\n",
    "# initializing the tensorbaord log writer\n",
    "writer = SummaryWriter(app_config[\"logs\"])\n",
    "\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, batched_test_data)\n",
    "\n",
    "    # save the model if validation loss decreases\n",
    "\n",
    "    if eval_loss < best_val_loss:\n",
    "        print(f\"eval perplexity : {math.exp(eval_loss)}\")\n",
    "        print(\"saving the model\")\n",
    "        best_val_loss = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "        directory_path = 'models'\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step, \n",
    "                'best_val_loss' : best_val_loss,\n",
    "            }, os.path.join(directory_path, 'best_model_wp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsoftmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4c44",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(best_model_path):\n",
    "    global model\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Preloading model {best_model_path}\")\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.load(best_model_path)\n",
    "        else:\n",
    "            state = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Model Not Found\")\n",
    "        \n",
    "loaded_model = loadModel(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2c2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = model\n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "        pred_text.append([vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        if(batch_size < 16):\n",
    "            gen_data = torch.cat((gen_data[:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words = 5,k=50):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.topk(output_softmax_permuted,k ,dim=2).indices.squeeze(0)\n",
    "        \n",
    "        values = torch.topk(softmax(output_softmax_permuted),k ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "\n",
    "        \n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "        \n",
    "\n",
    "        pred_text.append([vocab.lookup_token(next_index)][0])\n",
    "        if(batch_size < 15):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a0620cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "import math\n",
    "\n",
    "with open('models/morfessor_model.p','rb') as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8e650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_morph(l):\n",
    "    a = []\n",
    "    for v in l:\n",
    "        t1 = '-'.join(models[0].viterbi_segment(v)[0])\n",
    "        tr = re.sub(r'[ ]+', r' ', t1)\n",
    "        tr = re.sub(r'- -', r'*', tr)\n",
    "        tr = re.sub(r'-[ ]+', r'*', tr)\n",
    "        tr = re.sub(r'[ ]+-', r'*',tr)\n",
    "        tr = re.sub(r' ', r' * ',tr)\n",
    "        tr = re.sub(r'\\*', r' * ',tr)\n",
    "        tr = re.sub(r'  ', r' ', tr)\n",
    "        tr = re.sub(r'-', r' ',tr)\n",
    "        a.append(tr)\n",
    "    return a\n",
    "\n",
    "def revert_sentence(text):\n",
    "#     tr = re.sub(r' ', r'',text)\n",
    "    tr = re.sub(r'\\*', r' ',text)\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "772a4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = ['लामो समयसम्म प्रयोग गर्न सकिन्छ ।']\n",
    "# st = ['तपाईंलाई कस्तो पुस्तकहरू मन']\n",
    "st = ['नेपालमा आधुनिक']\n",
    "st = convert_to_morph(st)\n",
    "st_i = data_process(st)\n",
    "st_i = st_i.unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4149da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nonnaive_generator(loaded_model, st_i,no_words = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0958c4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नेपालमा * आधुनिक  <unk> भएका <unk>मात्र <unk> भएकोथियो समेत हो। विद्यार्थी <unk> <unk>?<unk>  ?हामीपनि ? ?<unk>? ?<unk> ? ?<unk> <unk> ?<unk> ? ?त्यो <unk>र निर्माण ?<unk> ? र  मन्त्रालयले रहेकोछ।उनले१वर्ष <unk> <unk> <unk> <unk> ? <unk> ? ? ?हामी ? '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st[0]+ revert_sentence(''.join(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c9571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
