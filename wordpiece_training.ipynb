{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4348b2f9",
   "metadata": {},
   "source": [
    "# Import Torch functions and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import preProcessText, getTokenizer,try_gpu , word_piece_decoder, word_piece_encoder\n",
    "from config import getConfig\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25f9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emsize': 300, 'd_hid': 800, 'nlayers': 4, 'nhead': 4, 'dropout': 0.05, 'bptt': 32}\n",
      "{'logs': 'tensorboard_logs', 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "model_config, app_config = getConfig(small = True)\n",
    "print(model_config)\n",
    "print(app_config)\n",
    "\n",
    "bptt=model_config[\"bptt\"]\n",
    "device = try_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba983f7",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  : data/preprocessed_word_piece.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/preprocessed_word_piece.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    with open('data/ne_dedup.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(\"Preprocessing file\")\n",
    "        text = preProcessText(text,tokenizer_type = 'word_piece')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "else:\n",
    "    print(f\"Reading file  : {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0761ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_t(text):\n",
    "    text = re.sub(r'\\u094D','a',text)\n",
    "    text = re.sub(r'\\u0941','u',text)\n",
    "    text = re.sub(r'\\u0942','e',text)\n",
    "    text = re.sub(r'\\u0901','i',text)\n",
    "    text = re.sub(r'\\u0902','c',text)\n",
    "    text = re.sub(r'\\u0943','r',text)\n",
    "    text = re.sub(r'\\u0947','l',text)\n",
    "    text = re.sub(r'\\u094b','o',text)\n",
    "    text = re.sub(r'\\u094c','p',text)\n",
    "    text = re.sub(r'\\u0948','k',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b02d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319566"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 10000\n",
    "\n",
    "train_iter_first = text.split('\\n')[:train_split]\n",
    "test_iter = text.split('\\n')[train_split:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "049958b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer,vocab = getTokenizer(tokenizer_type = 'word_piece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8092b820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = tokenizer.get_vocab()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdd18bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded $tring:  ['महान', '##ा', '##यक', 'राजlश', 'हमाल', 'अहिलl', 'चलचितaर', 'कaषlतaरमा', 'पात', '##लिए', '।']\n",
      "Decoded $tring: महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try the encoder and decoder\n",
    "l = tokenizer.encode(word_piece_encoder('महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।')).tokens\n",
    "l_ = tokenizer.encode(word_piece_encoder('महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए ।')).ids\n",
    "\n",
    "print(\"Encoded $tring: \",l)\n",
    "print(\"Decoded $tring:\",word_piece_decoder(tokenizer.decode(l_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8771c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded $tring:  ['हातमा', '[UNK]', 'ज', '##टा', '[UNK]', '[UNK]', 'बaरमaहा', 'उतa', '##पति', '[UNK]', '।']\n",
      "Encoded id$:  [4308, 1, 42, 307, 1, 1, 27723, 475, 826, 1, 77]\n",
      "Decoded $tring: हातमा जटा ब्रम्हा उत्पति ।\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try the encoder and decoder\n",
    "l = tokenizer.encode(word_piece_encoder('हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।')).tokens\n",
    "l_ = tokenizer.encode(word_piece_encoder('हातमा त्रिशुल जटा मुकुट शुशोभीत ब्रम्हा उत्पति हुनु ।')).ids\n",
    "\n",
    "print(\"Encoded $tring: \",l)\n",
    "print(\"Encoded id$: \",l_)\n",
    "print(\"Decoded $tring:\",word_piece_decoder(tokenizer.decode(l_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf967495",
   "metadata": {},
   "source": [
    "#  some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f7919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(l):\n",
    "    splitted_list = []\n",
    "    z = 0\n",
    "    for i,idx in enumerate(l):\n",
    "        if idx == 220:\n",
    "            splitted_list.append(l[z:i])\n",
    "            z = i+1\n",
    "    if z <= len(l)-1:\n",
    "        splitted_list.append(l[z:])\n",
    "    return splitted_list\n",
    "\n",
    "def splits_to_token(splited_list):\n",
    "    strings = [tokenizer.decode(l) for l in splited_list]\n",
    "    \n",
    "    return strings\n",
    "\n",
    "# print(tokenizer.encode(' ').ids)\n",
    "\n",
    "word_piece_decoder(tokenizer.decode([978,\n",
    " 261,\n",
    " 264,\n",
    "624,\n",
    " 261,\n",
    " 263]))\n",
    "\n",
    "k = split_list(l_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d435fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['हातमा जटा बaरमaहा उतaपति ।']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_to_token(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(tokenizer.encode(word_piece_encoder(item)).ids, dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "#     data = data.view(bsz, seq_len).t().contiguous()\n",
    "    data = data.view(bsz,seq_len).t()\n",
    "#     return data.to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "seq_length = 128\n",
    "import math\n",
    "\n",
    "\n",
    "def get_batch(source: Tensor, i: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(seq_length, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Split\n",
    "train_data = data_process(train_iter_first)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5674c13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733762"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b8f1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(len(train_iter_second),len(test_iter))\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d40be4",
   "metadata": {},
   "source": [
    "# Working with a dummy Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "\n",
    "\n",
    "text = ['आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "#text = ['जनसंख्या']\n",
    "sample_data = data_process(\n",
    "    text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16c88184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5522, 12438,  2404,     1,  3124, 13185,   177,     1,     1,   964,\n",
       "         1956,     1,  1373,  5110,  1922,     1,   964])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9de8a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5522,     1],\n",
       "        [12438,   964],\n",
       "        [ 2404,  1956],\n",
       "        [    1,     1],\n",
       "        [ 3124,  1373],\n",
       "        [13185,  5110],\n",
       "        [  177,  1922],\n",
       "        [    1,     1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = batchify(sample_data, 2)\n",
    "print(\"Given word:\", text[0])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd7744",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee046362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batched_train_data = batchify(train_data, bptt).to(device)  # shape [seq_len, batch_size]\n",
    "batched_test_data = batchify(test_data, bptt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6290c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, ntokens):\n",
    "    emsize = model_config[\"emsize\"]\n",
    "    d_hid = model_config[\"d_hid\"]\n",
    "    nlayers = model_config[\"nlayers\"]\n",
    "    nhead = model_config[\"nhead\"]\n",
    "    dropout = model_config[\"dropout\"]\n",
    "    model = TransformerModel(ntokens, emsize,nhead, d_hid, nlayers, dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4619fe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116425728"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)\n",
    "model = get_model(model_config, ntokens).to(device)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1da52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf7ec3",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "softmax = nn.Softmax(dim=2)\n",
    "#softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    global epoch\n",
    "    global global_step\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(batched_train_data) // bptt\n",
    "    progress_bar = tqdm(enumerate(range(0, batched_train_data.size(0) - 1, bptt)), total=num_batches, desc=f'Epoch {epoch}', ncols=80)\n",
    "    for batch_idx, i in progress_bar:\n",
    "        ### batch_idx -> (1, 2, 3, 4, ...)\n",
    "        ### i -> (0, bptt, 2*bptt, ....)\n",
    "        data, targets = get_batch(batched_train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ## calculate the postfix description for the progress bar\n",
    "        cur_loss = total_loss / (batch_idx + 1)\n",
    "        ppl = math.exp(cur_loss)\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": cur_loss, \"ppl\" : ppl}, refresh=True)\n",
    "        \n",
    "        writer.add_scalar('loss/train loss', cur_loss, global_step)\n",
    "        writer.flush()\n",
    "        writer.add_scalar('ppl/train perplexity', ppl, global_step)\n",
    "        writer.flush()\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(eval_data) // bptt\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(enumerate(range(0, eval_data.size(0) - 1, bptt)), total=num_batches, desc=f'Validation {epoch}', ncols=80)\n",
    "        for batch_idx, i in progress_bar:\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    \n",
    "    eval_loss = total_loss / (len(eval_data) - 1)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "\n",
    "    writer.add_scalar('loss/val loss', eval_loss, global_step)\n",
    "    writer.flush()\n",
    "    writer.add_scalar('ppl/val perplexity', eval_ppl, global_step)\n",
    "    writer.flush()\n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73f1d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08abd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'models/best_model_wp.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2670it [09:03,  4.91it/s, loss=5.34, ppl=208]                          \n",
      "Validation 0: 261it [00:20, 12.57it/s]                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval perplexity : 23193946812.60187\n",
      "saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2670it [09:04,  4.90it/s, loss=5.04, ppl=154]                          \n",
      "Validation 1: 261it [00:20, 12.59it/s]                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval perplexity : 17933978731.69122\n",
      "saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  55%|█████▌    | 1469/2669 [04:59<04:04,  4.90it/s, loss=4.84, ppl=127]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(app_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(initial_epoch, epochs):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, batched_test_data)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# save the model if validation loss decreases\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens), targets)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mF:\\LM bytepair\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\LM bytepair\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "best_val_loss = float('inf')\n",
    "initial_epoch = 0\n",
    "epochs = app_config[\"epochs\"]\n",
    "global_step = 0\n",
    "best_model = None\n",
    "\n",
    "# preload the model if exists to train more epochs\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Preloading model {best_model_path}\")\n",
    "    state = torch.load(best_model_path)\n",
    "    \n",
    "    initial_epoch = state['epoch'] + 1\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "    global_step = state['global_step']\n",
    "    best_val_loss = state['best_val_loss']\n",
    "    \n",
    "    print(initial_epoch, global_step, best_val_loss)\n",
    "\n",
    "# initializing the tensorbaord log writer\n",
    "writer = SummaryWriter(app_config[\"logs\"])\n",
    "\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, batched_test_data)\n",
    "\n",
    "    # save the model if validation loss decreases\n",
    "\n",
    "    if eval_loss < best_val_loss:\n",
    "        print(f\"eval perplexity : {math.exp(eval_loss)}\")\n",
    "        print(\"saving the model\")\n",
    "        best_val_loss = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "        directory_path = 'models'\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step, \n",
    "                'best_val_loss' : best_val_loss,\n",
    "            }, os.path.join(directory_path, 'best_model_wp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsoftmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33185f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_to_token(x):\n",
    "    token_list = [tokenizer.id_to_token(id) for id in x]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b999611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'r']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token([10,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4c44",
   "metadata": {},
   "source": [
    "# sample Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73966b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model models/best_model_wp.pt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Not Found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 9\u001b[0m, in \u001b[0;36mloadModel\u001b[1;34m(best_model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m         state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "def loadModel(best_model_path):\n",
    "    global model\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Preloading model {best_model_path}\")\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.load(best_model_path)\n",
    "        else:\n",
    "            state = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Model Not Found\")\n",
    "        \n",
    "loaded_model = loadModel(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2c2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "\n",
    "        pred_text.append(indices[0][-1])\n",
    "        if(batch_size < 128):\n",
    "            gen_data = torch.cat((gen_data[:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text\n",
    "\n",
    "\n",
    "\n",
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words = 5,k=50):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.topk(output_softmax_permuted,k ,dim=2).indices.squeeze(0)\n",
    "        \n",
    "        values = torch.topk(softmax(output_softmax_permuted),k ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "        \n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "\n",
    "        pred_text.append(next_index.item())\n",
    "        if(batch_size < 128):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c93c46c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]),\n",
       " tensor([[995],\n",
       "         [  1]], device='cuda:0'))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = ['नेपालमा आधुनिक']\n",
    "st_i = data_process(st)\n",
    "st_i = st_i.unsqueeze(1).to(device)\n",
    "st_i.shape,st_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08777245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नेपालमा आधुनिकमा र मिलेर बनेको छ । नेपाल उद्योग वाणिज्य महासंघले र वाणिज्य तथा आपूर्तिका लागि समेत छलफल गरिएको छ । यस्तै विभिन्न समेत आयोजना गर्ने गरी सम्झौता गरिएको'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = nonnaive_generator(loaded_model, st_i,no_words = 40, k=10)\n",
    "m = splits_to_token(split_list(z))\n",
    "word_piece_decoder(' '.join(st)+ ' '.join(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
